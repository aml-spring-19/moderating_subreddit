{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "\n",
    "# others\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# nlp\n",
    "import string\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/reddit_200k_train.csv', encoding = 'latin-1', index_col='Unnamed: 0')\n",
    "test = pd.read_csv('data/reddit_200k_test.csv', encoding = 'latin-1', index_col='Unnamed: 0')\n",
    "\n",
    "# subset the columns\n",
    "df['removed'] = df.REMOVED\n",
    "df = df[['body', 'removed']]\n",
    "\n",
    "test['removed'] = test.REMOVED\n",
    "test = test[['body', 'removed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Bag of Words and Simple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Baseline Model\n",
    "\n",
    "A simple logistic regression after vectorizing our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer() # initializing the vectorizer\n",
    "# fitting and transforming our text data - both train and test\n",
    "X_train_base = cv.fit_transform(df.body)\n",
    "X_test_base = cv.transform(test.body)\n",
    "\n",
    "# mapping our targets\n",
    "y_train = np.where(df.removed, 1, 0)\n",
    "y_test = np.where(test.removed, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag', random_state=42).fit(X_train_base, y_train)\n",
    "baseline_train_score = lr.score(X_train_base, y_train) # score on training set\n",
    "baseline_test_score = lr.score(X_test_base, y_test)    # score on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the performance on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BASELINE MODEL MEAN PERFORMANCE (ROC-AUC):')\n",
    "print('Training: {}'.format(np.round(np.mean(baseline_train_score), 2)))\n",
    "print('Test: {}'.format(np.round(np.mean(baseline_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(cv.get_feature_names())[np.argsort(lr.coef_[0])[:10]]; print('Bottom 10: {}'.format(bot10))\n",
    "top10 = np.array(cv.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:10]]; print('Top 10: {}'.format(top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Using lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try using lemmatization with the count vectorizer, which will help reduce the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the vectorizer w/ lemmatization\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "lem = CountVectorizer(tokenizer = LemmaTokenizer())  \n",
    "\n",
    "# transforming the text data with our new vectorizer\n",
    "X_train_lem = lem.fit_transform(df.body) # training\n",
    "X_test_lem = lem.transform(test.body)    # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag', random_state=42).fit(X_train_lem, y_train)\n",
    "\n",
    "# get the scores\n",
    "lem_train_score = lr.score(X_train_lem, y_train)  # training set\n",
    "lem_test_score = lr.score(X_test_lem, y_test)     # test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LEMMATIZATION MODEL MEAN PERFORMANCE (ROC-AUC):')\n",
    "print('Training: {}'.format(np.round(np.mean(lem_train_score), 2)))\n",
    "print('Test: {}'.format(np.round(np.mean(lem_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(lem.get_feature_names())[np.argsort(lr.coef_[0])[:10]]; print('Bottom 10: {}'.format(bot10))\n",
    "top10 = np.array(lem.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:10]]; print('Top 10: {}'.format(top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization makes our train and test scores worse, and doesn't seem to really be working given the names of the features here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Using tf-idf scaling w/ GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()                                            # tfidf vectorizer\n",
    "lr = LogisticRegression(solver='sag', random_state=42)               # model\n",
    "tfidf_pipeline = Pipeline([('preprocessing', tfidf), ('lr', lr)])    # pipeline - vectorizer and model\n",
    "param_grid = {'lr__C': np.logspace(-3, 2, 6)}                        # param grid\n",
    "\n",
    "# grid search\n",
    "gs = GridSearchCV(tfidf_pipeline, param_grid, cv=5, scoring='roc_auc').fit(df.body, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_score = gs.score(df.body, y_train) # training data\n",
    "tfidf_test_score = gs.score(test.body, y_test) # test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TFIDF MODEL MEAN PERFORMANCE (ROC-AUC):')\n",
    "print('Training: {}'.format(np.round(np.mean(tfidf_train_score), 2)))\n",
    "print('Test: {}'.format(np.round(np.mean(tfidf_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(gs.best_estimator_.steps[0][1].get_feature_names())[np.argsort(gs.best_estimator_.steps[1][1].coef_[0])[:10]]\n",
    "print('Bottom 10: {}'.format(bot10))\n",
    "top10 = np.array(gs.best_estimator_.steps[0][1].get_feature_names())[np.argsort(gs.best_estimator_.steps[1][1].coef_[0])[::-1][:10]]\n",
    "print('Top 10: {}'.format(top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf scaling gives us results that are slightly better than our baseline model. The features are also now much more interesting, especially the 30 features with the highest positive coefficients. Indeed, we find many words related to very sensitive subjects ('feminists', 'liberals', 'hillary', 'trump'), as well as curse words. Moreover, '0001f602' and '0001f914' actually correspond to emojis (laughing crying emoji and thinking emoji respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Using both lemmatization and tf-idf scaling w/ GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlem = TfidfVectorizer(tokenizer = LemmaTokenizer())                 # tfidf vectorizer w/ lemma\n",
    "lr = LogisticRegression(solver='sag', random_state=42)               # model\n",
    "tlem_pipeline = Pipeline([('preprocessing', tlem), ('lr', lr)])      # pipeline - vectorizer and model\n",
    "param_grid = {'lr__C': np.logspace(-3, 2, 6)}                        # param grid\n",
    "\n",
    "# grid search\n",
    "gs = GridSearchCV(tlem_pipeline, param_grid, cv=5, scoring='roc_auc').fit(df.body, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlem_train_score = gs.score(df.body, y_train) # training data\n",
    "tlem_test_score = gs.score(test.body, y_test) # test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TFIDF w/ LEMMA MODEL PERFORMANCE (ROC-AUC):')\n",
    "print('Training: {}'.format(np.round(np.mean(tfidf_train_score), 2)))\n",
    "print('Test: {}'.format(np.round(np.mean(tfidf_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(gs.best_estimator_.steps[0][1].get_feature_names())[np.argsort(gs.best_estimator_.steps[1][1].coef_[0])[:10]]\n",
    "print('Bottom 10: {}'.format(bot10))\n",
    "top10 = np.array(gs.best_estimator_.steps[0][1].get_feature_names())[np.argsort(gs.best_estimator_.steps[1][1].coef_[0])[::-1][:10]]\n",
    "print('Top 10: {}'.format(top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that while lemmatization made the baseline model worse, it actually makes tf-idf scaling better. However, the features are not really interpretable here and they seem to consist mostly of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Using bi-grams, tri-grams and 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "for w in ['no', 'not', 'how', 'why', 'himself', 'yourself', 'you', 'me']:\n",
    "    stopwords.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram = CountVectorizer(ngram_range=(2, 4), min_df=5, stop_words=stopwords)\n",
    "\n",
    "X_train_chng = gram.fit_transform(df.body)\n",
    "X_test_chng = gram.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_chng, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_train_score = lr.score(X_train_chng, y_train)\n",
    "chng_test_score = lr.score(X_test_chng, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model with n-grams achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(chng_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model with n-grams achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(chng_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(gram.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(gram.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the test score of n-grams is not so good, despite its training score being pretty high. However, we some other interesting features: some, such as \"comments removed\", may actually indicate a leak in the data. Others, like \"rick morty\", are... interesting! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using all of it: lemmatization, tf-idf scaling, n-grams w/ GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allv = CountVectorizer(ngram_range=(2, 4), min_df=5, stop_words='english', tokenizer=LemmaTokenizer()) # tfidf vectorizer w/ lemma\n",
    "lr = LogisticRegression(solver='sag', random_state=42)                                                 # model\n",
    "allv_pipeline = Pipeline([('preprocessing', allv), ('lr', lr)])                                        # pipeline - vectorizer and model\n",
    "param_grid = {'lr__C': np.logspace(-3, 2, 6)}                                                          # param grid\n",
    "\n",
    "# grid search\n",
    "gs = GridSearchCV(allv_pipeline, param_grid, cv=5, scoring='roc_auc').fit(df.body, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allv_train_score = gs.score(df.body, y_train) # training data\n",
    "allv_test_score = gs.score(test.body, y_test) # test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TFIDF w/ LEMMA MODEL PERFORMANCE (ROC-AUC):')\n",
    "print('Training: {}'.format(np.round(np.mean(allv_train_score), 2)))\n",
    "print('Test: {}'.format(np.round(np.mean(allv_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(gs.best_estimator_.steps[0][1].get_feature_names())[np.argsort(gs.best_estimator_.steps[1][1].coef_[0])[:10]]\n",
    "print('Bottom 10: {}'.format(bot10))\n",
    "top10 = np.array(gs.best_estimator_.steps[0][1].get_feature_names())[np.argsort(gs.best_estimator_.steps[1][1].coef_[0])[::-1][:10]]\n",
    "print('Top 10: {}'.format(top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining everything actually seems to yield the worst scores so far, which is somewhat surprising. The Lemma Tokenizer probably doesn't really work as we'd intend it to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Other features\n",
    "\n",
    "We'll engineer the following features:\n",
    "- Length: document size (# of characters)\n",
    "- Capitalization: percentage of capital characters\n",
    "- Punctuations: boolean indicating whether the post contained punctuations or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df.body.str.len()\n",
    "test['length'] = test.body.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upper Case Characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_cap'] = np.where(df.body.str.isupper(), 1, 0)\n",
    "test['all_cap'] = np.where(test.body.str.isupper(), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['punctuation'] = np.where(df.body.str.contains('!'), 1, 0)\n",
    "test['punctuation'] = np.where(test.body.str.contains('!'), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit((df['length']).values.reshape(-1,1))\n",
    "df['length'] = scaler.transform(df['length'].values.reshape(-1,1))\n",
    "test['length'] = scaler.transform(test['length'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlem = TfidfVectorizer(tokenizer = LemmaTokenizer())  # vectorizer w/ lemma\n",
    "\n",
    "X_tlem = tlem.fit_transform(df.body)                  # transform train\n",
    "X_test_tlem = tlem.transform(test.body)               # transform test\n",
    "\n",
    "# combining the text data with the other features\n",
    "X_tlem = hstack((X_tlem, df[['length', 'all_cap', 'punctuation']].values))\n",
    "X_test_tlem = hstack((X_test_tlem, test[['length', 'all_cap', 'punctuation']].values))\n",
    "\n",
    "# training the model\n",
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_tlem, y_train)\n",
    "tlem_score = lr.score(X_tlem, y_train)\n",
    "tlem_test_score = lr.score(X_test_tlem, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TFIDF w/ LEMMA and EXTRA FEATURES PERFORMANCE (ROC-AUC):')\n",
    "print('Training: {}'.format(np.round(np.mean(tlem_score), 2)))\n",
    "print('Test: {}'.format(np.round(np.mean(tlem_test_score), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = models.KeyedVectors.load_word2vec_format('V:/word_vectors/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing our text body and the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_w2v = CountVectorizer(vocabulary=w.index2word)\n",
    "vect_w2v.fit(df.body)\n",
    "\n",
    "docs = vect_w2v.inverse_transform(vect_w2v.transform(df.body))\n",
    "X_train_body = []\n",
    "for doc in docs:\n",
    "    if len(doc) > 0:\n",
    "        X_train_body.append(np.mean(w[doc], axis=0))\n",
    "    else:\n",
    "        X_train_body.append(np.zeros(300))\n",
    "X_train_body = np.vstack(X_train_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating the above for the test set\n",
    "docs_test = vect_w2v.inverse_transform(vect_w2v.transform(test.body))\n",
    "X_test_body = []\n",
    "for doc in docs_test:\n",
    "    if len(doc) > 0:\n",
    "        X_test_body.append(np.mean(w[doc], axis=0))\n",
    "    else:\n",
    "        X_test_body.append(np.zeros(300))\n",
    "X_test_body = np.vstack(X_test_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_body, y_train)\n",
    "\n",
    "w2v_train_score = lr.score(X_train_body, y_train)\n",
    "w2v_test_score = lr.score(X_test_body, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model w/ W2V achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(w2v_train_score), 2)))\n",
    "print('Model w/ W2V achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(w2v_test_score), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we incorporate the other features? Including one that indicates that there were no vocab words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_series = pd.Series(docs)\n",
    "df['v_length'] = docs_series.apply(lambda x: len(x)) # finds the document length\n",
    "df['v_empty'] = np.where(df.v_length == 0.0, 1, 0)   # maps empty docs to 1 and others to 0\n",
    "\n",
    "# repeat the above for test\n",
    "docs_series = pd.Series(docs_test)\n",
    "test['v_length'] = docs_series.apply(lambda x: len(x)) # finds the document length\n",
    "test['v_empty'] = np.where(test.v_length == 0.0, 1, 0)   # maps empty docs to 1 and others to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_body2 = np.concatenate((X_train_body, df[['length', 'all_cap', 'v_empty', 'punctuation']].values), axis=1)\n",
    "X_test_body2 = np.concatenate((X_test_body, test[['length', 'all_cap', 'v_empty', 'punctuation']].values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_body2, y_train)\n",
    "\n",
    "w2v_train_score2 = lr.score(X_train_body2, y_train)\n",
    "w2v_test_score2 = lr.score(X_test_body2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Second model w/ W2V achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(w2v_train_score), 2)))\n",
    "print('Second model w/ W2V achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(w2v_test_score), 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
