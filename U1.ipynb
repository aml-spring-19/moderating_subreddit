{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\costa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\costa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\costa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# others\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# nlp\n",
    "import string\n",
    "from gensim import models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/reddit_200k_train.csv', encoding = 'latin-1', index_col='Unnamed: 0')\n",
    "test = pd.read_csv('data/reddit_200k_test.csv', encoding = 'latin-1', index_col='Unnamed: 0')\n",
    "\n",
    "# subset the columns\n",
    "df['removed'] = df.REMOVED\n",
    "df = df[['body', 'removed']]\n",
    "\n",
    "test['removed'] = test.REMOVED\n",
    "test = test[['body', 'removed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've always been taught it emerged from the ea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  removed\n",
       "1  I've always been taught it emerged from the ea...    False\n",
       "2  As an ECE, my first feeling as \"HEY THAT'S NOT...     True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Bag of Words and Simple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_base = cv.fit_transform(df.body)\n",
    "y_train = np.where(df.removed, 1, 0)\n",
    "\n",
    "X_test_base = cv.transform(test.body)\n",
    "y_test = np.where(test.removed, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_base, y_train)\n",
    "baseline_train_score = lr.score(X_train_base, y_train)\n",
    "baseline_test_score = lr.score(X_test_base, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model achieves a mean of 0.75 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(baseline_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model achieves a mean of 0.75 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(baseline_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(cv.get_feature_names())[np.argsort(lr.coef_[0])[:10]]\n",
    "top10 = np.array(cv.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iâ', 'itâ', 'donâ', 'http', 'edit', 'does', 'www', 'link', 'org',\n",
       "       'com'], dtype='<U252')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fuck', 'comments', 'removed', 'shit', 'women', 'oh', 'weed', 'my',\n",
       "       'comment', 'let'], dtype='<U252')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Using lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try using lemmatization with the count vectorizer, which will help reduce the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "lem = CountVectorizer(tokenizer = LemmaTokenizer())\n",
    "X_train_lem = lem.fit_transform(df.body)\n",
    "X_test_lem = lem.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_lem, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_train_score = lr.score(X_train_lem, y_train)\n",
    "lem_test_score = lr.score(X_test_lem, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with lemmatization achieves a mean of 0.72 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with lemmatization achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(lem_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with lemmatization achieves a mean of 0.72 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with lemmatization achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(lem_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(lem.get_feature_names())[np.argsort(lr.coef_[0])[:10]]\n",
    "top10 = np.array(lem.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['?', ':', 'http', 'how', 'would', 'what', 'itâ\\x80\\x99s', 'there',\n",
       "       'in', 'doe'], dtype='<U830')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['my', '!', 'comment', 'me', 'it\\x92s', 'woman', '...', 'removed',\n",
       "       'fuck', '<'], dtype='<U830')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization makes our train and test scores worse, and doesn't seem to really be working given the names of the features here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Using tf-idf scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(df.body)\n",
    "X_test_tfidf = tfidf.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_score = lr.score(X_train_tfidf, y_train)\n",
    "tfidf_test_score = lr.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling achieves a mean of 0.81 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(tfidf_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling achieves a mean of 0.78 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(tfidf_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(tfidf.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(tfidf.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iâ', 'itâ', 'donâ', 'edit', '½ï', 'thatâ', 'http', 'doesnâ',\n",
       "       'www', 'isnâ', 'youâ', 'didnâ', 'org', 'theyâ', 'canâ', 'https',\n",
       "       'abstract', 'weâ', 'doi', 'hi', 'thereâ', 'arenâ', 'eli5', 'link',\n",
       "       'question', 'curious', 'com', 'or', 'does', 'similar'],\n",
       "      dtype='<U252')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fuck', 'mods', 'my', 'comments', '0001f914', '0001f602', 'flair',\n",
       "       'censorship', 'removed', 'ass', 'wet', 'women', 'pharma',\n",
       "       'thumbnail', 'upvoted', 'vote', 'upvote', 'genders', 'liberals',\n",
       "       'turtle', 'weed', 'feminists', 'vegans', 'lsd', 'hillary',\n",
       "       'saffron', 'racist', 'fe0f', 'shit', 'trump'], dtype='<U252')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf scaling gives us results that are slightly better than our baseline model. The features are also now much more interesting, especially the 30 features with the highest positive coefficients. Indeed, we find many words related to very sensitive subjects ('feminists', 'liberals', 'hillary', 'trump'), as well as curse words. Moreover, '0001f602' and '0001f914' actually correspond to emojis (laughing crying emoji and thinking emoji respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Using both lemmatization and tf-idf scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlem = TfidfVectorizer(tokenizer = LemmaTokenizer())\n",
    "\n",
    "X_train_tlem = tlem.fit_transform(df.body)\n",
    "X_test_tlem = tlem.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_tlem, y_train)\n",
    "\n",
    "tlem_train_score = lr.score(X_train_tlem, y_train)\n",
    "tlem_test_score = lr.score(X_test_tlem, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling and lemmatization achieves a mean of 0.87 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling and lemmatization achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(tlem_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling and lemmatization achieves a mean of 0.79 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling and lemmatization achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(tlem_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(tlem.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(tlem.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it\\x92s', 'don\\x92t', 'i\\x92m', 'can\\x92t', 'doesn\\x92t',\n",
       "       'that\\x92s', 'i\\x92ve', '\\x94', 'didn\\x92t', 'isn\\x92t', '<', '>',\n",
       "       'they\\x92re', 'you\\x92re', 'there\\x92s', 'i\\x92ll', 'i\\x92d',\n",
       "       'we\\x92re', '\\x96', 'won\\x92t', 'what\\x92s', 'wouldn\\x92t',\n",
       "       'let\\x92s', 'aren\\x92t', '\\x93the', '\\x97', 'couldn\\x92t',\n",
       "       'we\\x92ve', 'wasn\\x92t', 'upvote'], dtype='<U830')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['itâ\\x80\\x99s', 'donâ\\x80\\x99t', 'iâ\\x80\\x99m', 'edit',\n",
       "       'thatâ\\x80\\x99s', 'doesnâ\\x80\\x99t', 'ï¿½ï¿½', 'iâ\\x80\\x99ve',\n",
       "       'didnâ\\x80\\x99t', 'canâ\\x80\\x99t', 'â\\x80\\x9d', 'http',\n",
       "       'isnâ\\x80\\x99t', 'arenâ\\x80\\x99t', 'thereâ\\x80\\x99s',\n",
       "       'youâ\\x80\\x99re', 'theyâ\\x80\\x99re', 'iâ\\x80\\x99ll', 'iâ\\x80\\x99d',\n",
       "       'wouldnâ\\x80\\x99t', 'weâ\\x80\\x99re', 'â\\x80\\x94', 'wonâ\\x80\\x99t',\n",
       "       'whatâ\\x80\\x99s', 'wasnâ\\x80\\x99t', 'havenâ\\x80\\x99t', 'abstract',\n",
       "       'hi', 'letâ\\x80\\x99s', 'couldnâ\\x80\\x99t'], dtype='<U830')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that while lemmatization made the baseline model worse, it actually makes tf-idf scaling better. However, the features are not really interpretable here and they seem to consist mostly of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Using bi-grams, tri-grams and 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "for w in ['no', 'not', 'how', 'why', 'himself', 'yourself', 'you', 'me']:\n",
    "    stopwords.remove(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram = CountVectorizer(ngram_range=(2, 4), min_df=5, stop_words=stopwords)\n",
    "\n",
    "X_train_chng = gram.fit_transform(df.body)\n",
    "X_test_chng = gram.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_chng, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_train_score = lr.score(X_train_chng, y_train)\n",
    "chng_test_score = lr.score(X_test_chng, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with n-grams achieves a mean of 0.83 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with n-grams achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(chng_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with n-grams achieves a mean of 0.71 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with n-grams achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(chng_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(gram.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(gram.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['itâ not', 'thanks ama', 'iâ not', 'donâ know', 'donâ think',\n",
       "       'you donâ', 'anyone know', 'abstract gt', 'link paper',\n",
       "       'you recommend', 'link abstract', 'someone explain', 'org content',\n",
       "       'someone eli5', 'people donâ', 'you canâ', 'iâ sure',\n",
       "       'stupid question', 'interested see', 'thereâ no', 'hi dr',\n",
       "       'press release', 'how could', 'thatâ why', 'near future',\n",
       "       'machine learning', 'speed light', 'remember reading',\n",
       "       'solar panels', '½ï ½ï'], dtype='<U83')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comment section', 'comments removed', 'fuck you', 'fat shaming',\n",
       "       'big pharma', 'comments deleted', 'social justice',\n",
       "       'happened comments', 'affirmative action', 'commit suicide',\n",
       "       'you saying', 'no shit', 'great tits', 'rick morty', 'well shit',\n",
       "       'everything removed', 'vote republican', 'oh no', 'finds way',\n",
       "       'comment removed', 'oh wait', 'white man', 'fat acceptance',\n",
       "       'call me', 'without reading', 'oh god', '18 years', 'sea turtle',\n",
       "       'fake news', 'many removed'], dtype='<U83')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the test score of n-grams is not so good, despite its training score being pretty high. However, we some other interesting features: some, such as \"comments removed\", may actually indicate a leak in the data. Others, like \"rick morty\", are... interesting! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using all of it: lemmatization, tf-idf scaling, n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "allv = CountVectorizer(ngram_range=(2, 4), min_df=5, stop_words='english', tokenizer=LemmaTokenizer())\n",
    "\n",
    "X_train_all = allv.fit_transform(df.body)\n",
    "X_test_all = allv.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_score = lr.score(X_train_all, y_train)\n",
    "all_test_score = lr.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with everything achieves a mean of 0.69 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with everything achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(all_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with everything achieves a mean of 0.68 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with everything achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(all_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(allv.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(allv.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http :', ') .', 'edit :', ': http', ': http :', '? ,', '. ,',\n",
       "       \"doe n't\", '& gt', '& gt ;', 'gt ;', ', doe', ') ,', '. edit',\n",
       "       \", 's\", '. think', 'climate change', '. edit :', '? doe',\n",
       "       'question :', '. doe', '] (', ') ?', '( http :', '( http',\n",
       "       '. itâ\\x80\\x99s', \", n't\", '] ( http :', '] ( http', 'doing ama'],\n",
       "      dtype='<U142')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['> <', 'comment removed', '. it\\x92s', '! !', 'removed ?',\n",
       "       '. i\\x92m', ', it\\x92s', '. don\\x92t', \"! ''\", 'comment removed ?',\n",
       "       'shit .', 'high school', 'fuck .', '. fuck', '? !',\n",
       "       'comment section', '! ! !', 'gon na', '. wa', 'yeah ,', ', i\\x92m',\n",
       "       \"n't want\", '... ...', 'big pharma', ', don\\x92t',\n",
       "       'comment deleted', '. <', '. started', '. woman', 'u+0001f602 >'],\n",
       "      dtype='<U142')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining everything actually seems to yield the worst scores so far, which is somewhat surprising. The Lemma Tokenizer probably doesn't really work as we'd intend it to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Other features\n",
    "\n",
    "We'll engineer the following features:\n",
    "- Length: document size (# of characters)\n",
    "- Capitalization: percentage of capital characters\n",
    "- Punctuations: boolean indicating whether the post contained punctuations or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've always been taught it emerged from the ea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  removed\n",
       "1  I've always been taught it emerged from the ea...    False\n",
       "2  As an ECE, my first feeling as \"HEY THAT'S NOT...     True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df.body.str.len()\n",
    "test['length'] = test.body.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upper Case Characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_cap'] = np.where(df.body.str.isupper(), 1, 0)\n",
    "test['all_cap'] = np.where(test.body.str.isupper(), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['punctuation'] = np.where(df.body.str.contains('!'), 1, 0)\n",
    "test['punctuation'] = np.where(test.body.str.contains('!'), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = StandardScaler().fit_transform(df['length'].values.reshape(-1,1))\n",
    "test['length'] = StandardScaler().fit_transform(test['length'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfVectorizer(tokenizer = LemmaTokenizer(), stop_words = stopwords)\n",
    "X_model = model.fit_transform(df.body)\n",
    "X_test_model = model.transform(test.body)\n",
    "\n",
    "# combining the text data with the other features\n",
    "X_model = hstack((X_model, df[['length', 'all_cap', 'punctuation']].values))\n",
    "X_test_model = hstack((X_test_model, test[['length', 'all_cap', 'punctuation']].values))\n",
    "\n",
    "# training the model\n",
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_model, y_train)\n",
    "model_score = lr.score(X_model, y_train)\n",
    "model_test_score = lr.score(X_test_model, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with everything achieves a mean of 0.83 ROC-AUC on our training data.\n",
      "Model with everything achieves a mean of 0.79 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with extra features achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(model_score), 2)))\n",
    "print('Model with extra features achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(model_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying the same thing without the stopwords change\n",
    "model = TfidfVectorizer(tokenizer = LemmaTokenizer())\n",
    "\n",
    "X_model = model.fit_transform(df.body)\n",
    "X_test_model = model.transform(test.body)\n",
    "\n",
    "X_model = hstack((X_model, df[['length', 'all_cap', 'punctuation']].values))\n",
    "X_test_model = hstack((X_test_model, test[['length', 'all_cap', 'punctuation']].values))\n",
    "\n",
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_model, y_train)\n",
    "model_score = lr.score(X_model, y_train)\n",
    "model_test_score = lr.score(X_test_model, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with everything achieves a mean of 0.83 ROC-AUC on our training data.\n",
      "Model with everything achieves a mean of 0.79 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with extra features achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(model_score), 2)))\n",
    "print('Model with extra features achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(model_test_score), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = models.KeyedVectors.load_word2vec_format('V:/word_vectors/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing our text body and the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_w2v = CountVectorizer(vocabulary=w.index2word)\n",
    "vect_w2v.fit(df.body)\n",
    "\n",
    "docs = vect_w2v.inverse_transform(vect_w2v.transform(df.body))\n",
    "X_train_body = []\n",
    "for doc in docs:\n",
    "    if len(doc) > 0:\n",
    "        X_train_body.append(np.mean(w[doc], axis=0))\n",
    "    else:\n",
    "        X_train_body.append(np.zeros(300))\n",
    "X_train_body = np.vstack(X_train_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeating the above for the test set\n",
    "docs_test = vect_w2v.inverse_transform(vect_w2v.transform(test.body))\n",
    "X_test_body = []\n",
    "for doc in docs_test:\n",
    "    if len(doc) > 0:\n",
    "        X_test_body.append(np.mean(w[doc], axis=0))\n",
    "    else:\n",
    "        X_test_body.append(np.zeros(300))\n",
    "X_test_body = np.vstack(X_test_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_body, y_train)\n",
    "\n",
    "w2v_train_score = lr.score(X_train_body, y_train)\n",
    "w2v_test_score = lr.score(X_test_body, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model w/ W2V achieves a mean of 0.73 ROC-AUC on our training data.\n",
      "Model w/ W2V achieves a mean of 0.73 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model w/ W2V achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(w2v_train_score), 2)))\n",
    "print('Model w/ W2V achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(w2v_test_score), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we incorporate the other features? Including one that indicates that there were no vocab words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_series = pd.Series(docs)\n",
    "df['v_length'] = docs_series.apply(lambda x: len(x)) # finds the document length\n",
    "df['v_empty'] = np.where(df.v_length == 0.0, 1, 0)   # maps empty docs to 1 and others to 0\n",
    "\n",
    "# repeat the above for test\n",
    "docs_series = pd.Series(docs_test)\n",
    "test['v_length'] = docs_series.apply(lambda x: len(x)) # finds the document length\n",
    "test['v_empty'] = np.where(test.v_length == 0.0, 1, 0)   # maps empty docs to 1 and others to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_body2 = np.concatenate((X_train_body, df[['length', 'all_cap', 'v_empty', 'punctuation']].values), axis=1)\n",
    "X_test_body2 = np.concatenate((X_test_body, test[['length', 'all_cap', 'v_empty', 'punctuation']].values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_body2, y_train)\n",
    "\n",
    "w2v_train_score2 = lr.score(X_train_body2, y_train)\n",
    "w2v_test_score2 = lr.score(X_test_body2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second model w/ W2V achieves a mean of 0.73 ROC-AUC on our training data.\n",
      "Second model w/ W2V achieves a mean of 0.73 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Second model w/ W2V achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(w2v_train_score), 2)))\n",
    "print('Second model w/ W2V achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(w2v_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
