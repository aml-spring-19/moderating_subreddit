{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# others\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\coren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\coren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/reddit_200k_train.csv', encoding = 'latin-1', index_col='Unnamed: 0')\n",
    "test = pd.read_csv('data/reddit_200k_test.csv', encoding = 'latin-1', index_col='Unnamed: 0')\n",
    "\n",
    "# subset the columns\n",
    "df['removed'] = df.REMOVED\n",
    "df = df[['body', 'removed']]\n",
    "\n",
    "test['removed'] = test.REMOVED\n",
    "test = test[['body', 'removed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've always been taught it emerged from the ea...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As an ECE, my first feeling as \"HEY THAT'S NOT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  removed\n",
       "1  I've always been taught it emerged from the ea...    False\n",
       "2  As an ECE, my first feeling as \"HEY THAT'S NOT...     True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Bag of Words and Simple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_base = cv.fit_transform(df.body)\n",
    "y_train = np.where(df.removed, 1, 0)\n",
    "\n",
    "X_test_base = cv.transform(test.body)\n",
    "y_test = np.where(test.removed, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_base, y_train)\n",
    "baseline_train_score = lr.score(X_train_base, y_train)\n",
    "baseline_test_score = lr.score(X_test_base, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model achieves a mean of 0.75 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(baseline_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model achieves a mean of 0.75 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(baseline_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(cv.get_feature_names())[np.argsort(lr.coef_[0])[:10]]\n",
    "top10 = np.array(cv.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iâ', 'itâ', 'donâ', 'http', 'edit', 'does', 'www', 'link', 'org',\n",
       "       'com'], dtype='<U252')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fuck', 'comments', 'removed', 'shit', 'women', 'oh', 'weed', 'my',\n",
       "       'comment', 'let'], dtype='<U252')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Using lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try using lemmatization with the count vectorizer, which will help reduce the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "lem = CountVectorizer(tokenizer = LemmaTokenizer())\n",
    "X_train_lem = lem.fit_transform(df.body)\n",
    "X_test_lem = lem.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_lem, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_train_score = lr.score(X_train_lem, y_train)\n",
    "lem_test_score = lr.score(X_test_lem, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with lemmatization achieves a mean of 0.72 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with lemmatization achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(lem_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with lemmatization achieves a mean of 0.72 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with lemmatization achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(lem_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot10 = np.array(lem.get_feature_names())[np.argsort(lr.coef_[0])[:10]]\n",
    "top10 = np.array(lem.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['?', ':', 'http', 'how', 'would', 'what', 'itâ\\x80\\x99s', 'there',\n",
       "       'in', 'doe'], dtype='<U830')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['my', '!', 'comment', 'me', 'it\\x92s', 'woman', '...', 'removed',\n",
       "       'fuck', '<'], dtype='<U830')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization makes our train and test scores worse, and doesn't seem to really be working given the names of the features here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Using tf-idf scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(df.body)\n",
    "X_test_tfidf = tfidf.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_score = lr.score(X_train_tfidf, y_train)\n",
    "tfidf_test_score = lr.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling achieves a mean of 0.81 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(tfidf_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling achieves a mean of 0.78 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(tfidf_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(tfidf.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(tfidf.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iâ', 'itâ', 'donâ', 'edit', '½ï', 'thatâ', 'http', 'doesnâ',\n",
       "       'www', 'isnâ', 'youâ', 'didnâ', 'org', 'theyâ', 'canâ', 'https',\n",
       "       'abstract', 'weâ', 'doi', 'hi', 'thereâ', 'arenâ', 'eli5', 'link',\n",
       "       'question', 'curious', 'com', 'or', 'does', 'similar'],\n",
       "      dtype='<U252')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fuck', 'mods', 'my', 'comments', '0001f914', '0001f602', 'flair',\n",
       "       'censorship', 'removed', 'ass', 'wet', 'women', 'pharma',\n",
       "       'thumbnail', 'upvoted', 'vote', 'upvote', 'genders', 'liberals',\n",
       "       'turtle', 'weed', 'feminists', 'vegans', 'lsd', 'hillary',\n",
       "       'saffron', 'racist', 'fe0f', 'shit', 'trump'], dtype='<U252')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf scaling gives us results that are slightly better than our baseline model. The features are also now much more interesting, especially the 30 features with the highest positive coefficients. Indeed, we find many words related to very sensitive subjects ('feminists', 'liberals', 'hillary', 'trump'), as well as curse words. Moreover, '0001f602' and '0001f914' actually correspond to emojis (laughing crying emoji and thinking emoji respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Using both lemmatization and tf-idf scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlem = TfidfVectorizer(tokenizer = LemmaTokenizer())\n",
    "\n",
    "X_train_tlem = tlem.fit_transform(df.body)\n",
    "X_test_tlem = tlem.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_tlem, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlem_train_score = lr.score(X_train_tlem, y_train)\n",
    "tlem_test_score = lr.score(X_test_tlem, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling and lemmatization achieves a mean of 0.87 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling and lemmatization achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(tlem_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with tf-idf scaling and lemmatization achieves a mean of 0.79 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with tf-idf scaling and lemmatization achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(tlem_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(tlem.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(tlem.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['it\\x92s', 'don\\x92t', 'i\\x92m', 'can\\x92t', 'doesn\\x92t',\n",
       "       'that\\x92s', 'i\\x92ve', '\\x94', 'didn\\x92t', 'isn\\x92t', '<', '>',\n",
       "       'they\\x92re', 'you\\x92re', 'there\\x92s', 'i\\x92ll', 'i\\x92d',\n",
       "       'we\\x92re', '\\x96', 'won\\x92t', 'what\\x92s', 'wouldn\\x92t',\n",
       "       'let\\x92s', 'aren\\x92t', '\\x93the', '\\x97', 'couldn\\x92t',\n",
       "       'we\\x92ve', 'wasn\\x92t', 'upvote'], dtype='<U830')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['itâ\\x80\\x99s', 'donâ\\x80\\x99t', 'iâ\\x80\\x99m', 'edit',\n",
       "       'thatâ\\x80\\x99s', 'doesnâ\\x80\\x99t', 'ï¿½ï¿½', 'iâ\\x80\\x99ve',\n",
       "       'didnâ\\x80\\x99t', 'canâ\\x80\\x99t', 'â\\x80\\x9d', 'http',\n",
       "       'isnâ\\x80\\x99t', 'arenâ\\x80\\x99t', 'thereâ\\x80\\x99s',\n",
       "       'youâ\\x80\\x99re', 'theyâ\\x80\\x99re', 'iâ\\x80\\x99ll', 'iâ\\x80\\x99d',\n",
       "       'wouldnâ\\x80\\x99t', 'weâ\\x80\\x99re', 'â\\x80\\x94', 'wonâ\\x80\\x99t',\n",
       "       'whatâ\\x80\\x99s', 'wasnâ\\x80\\x99t', 'havenâ\\x80\\x99t', 'abstract',\n",
       "       'hi', 'letâ\\x80\\x99s', 'couldnâ\\x80\\x99t'], dtype='<U830')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that while lemmatization made the baseline model worse, it actually makes tf-idf scaling better. However, the features are not really interpretable here and they seem to consist mostly of stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Using bi-grams, tri-grams and 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram = CountVectorizer(ngram_range=(2, 4), min_df=5, stop_words='english')\n",
    "\n",
    "X_train_chng = gram.fit_transform(df.body)\n",
    "X_test_chng = gram.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_chng, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chng_train_score = lr.score(X_train_chng, y_train)\n",
    "chng_test_score = lr.score(X_test_chng, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with n-grams achieves a mean of 0.81 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with n-grams achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(chng_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with n-grams achieves a mean of 0.69 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with n-grams achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(chng_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(gram.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(gram.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['iâ ve', 'iâ ll', 'donâ know', 'thanks doing', 'thanks ama',\n",
       "       'donâ think', 'doing ama', 'iâ sure', 'hi dr', 'weâ ve',\n",
       "       'abstract gt', 'itâ just', 'itâ like', 'link paper', 'does know',\n",
       "       'correct wrong', 'stupid question', 'thank ama', 'near future',\n",
       "       'machine learning', 'theyâ ll', 'thank time', 'thank doing',\n",
       "       'org content', 'gut microbiome', 'speed light', 'link abstract',\n",
       "       'people donâ', 'solar panels', 'remember reading'], dtype='<U86')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comments removed', 'comment section', 'comments deleted',\n",
       "       'comment removed', 'happened comments', 'don fuck', 'big pharma',\n",
       "       'fat shaming', 'social justice', 'don tell', 'commit suicide',\n",
       "       'water wet', 'affirmative action', 'great tits', 'finds way',\n",
       "       'thread removed', 'rick morty', 'vote republican', 'white man',\n",
       "       'oh god', 'removed posts', 'removed comments', 'removing comments',\n",
       "       'wind power', 'oh wait', 'fat acceptance', 'healthy size',\n",
       "       'comments section', 'fake news', 'don let'], dtype='<U86')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the test score of n-grams is not so good, despite its training score being pretty high. However, we some other interesting features: some, such as \"comments removed\", may actually indicate a leak in the data. Others, like \"rick morty\", are... interesting! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using all of it: lemmatization, tf-idf scaling, n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "allv = CountVectorizer(ngram_range=(2, 4), min_df=5, stop_words='english', tokenizer=LemmaTokenizer())\n",
    "\n",
    "X_train_all = allv.fit_transform(df.body)\n",
    "X_test_all = allv.transform(test.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionCV(cv=5, scoring='roc_auc', solver='sag').fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_score = lr.score(X_train_all, y_train)\n",
    "all_test_score = lr.score(X_test_all, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with everything achieves a mean of 0.69 ROC-AUC on our training data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with everything achieves a mean of {} ROC-AUC on our training data.'.format(np.round(np.mean(all_train_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with everything achieves a mean of 0.68 ROC-AUC on our test data.\n"
     ]
    }
   ],
   "source": [
    "print('Model with everything achieves a mean of {} ROC-AUC on our test data.'.format(np.round(np.mean(all_test_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot30 = np.array(allv.get_feature_names())[np.argsort(lr.coef_[0])[:30]]\n",
    "top30 = np.array(allv.get_feature_names())[np.argsort(lr.coef_[0])[::-1][:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http :', ') .', 'edit :', ': http', ': http :', '? ,', '. ,',\n",
       "       \"doe n't\", '& gt', '& gt ;', 'gt ;', ', doe', ') ,', '. edit',\n",
       "       \", 's\", '. think', 'climate change', '. edit :', '? doe',\n",
       "       'question :', '. doe', '] (', ') ?', '( http :', '( http',\n",
       "       '. itâ\\x80\\x99s', \", n't\", '] ( http :', '] ( http', 'doing ama'],\n",
       "      dtype='<U142')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['> <', 'comment removed', '. it\\x92s', '! !', 'removed ?',\n",
       "       '. i\\x92m', ', it\\x92s', '. don\\x92t', \"! ''\", 'comment removed ?',\n",
       "       'shit .', 'high school', 'fuck .', '. fuck', '? !',\n",
       "       'comment section', '! ! !', 'gon na', '. wa', 'yeah ,', ', i\\x92m',\n",
       "       \"n't want\", '... ...', 'big pharma', ', don\\x92t',\n",
       "       'comment deleted', '. <', '. started', '. woman', 'u+0001f602 >'],\n",
       "      dtype='<U142')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining everything actually seems to yield the worst scores so far, which is somewhat surprising. The Lemma Tokenizer probably doesn't really work as we'd intend it to. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
